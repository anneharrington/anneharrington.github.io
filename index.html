<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Anne Harrington</title>

    <meta name="author" content="Anne Harrington">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Anne Harrington
                </p>
                <p>I'm CTO and Co-founder of <a href="https://yoku-ai.com">Yoku AI</a>, a start-up building healthcare and emotional communication tools. Using my background in computer vision and visual perception, I am leading the development of a robust stress and emotion detection app. Check out Yoku AI's pitch from MIT's delta v accelerator <a href="https://vimeo.com/863215344">here!</a>
                </p>
                <p>
                   I did my undergrad and a master's at MIT, receiving a joint degree in Computer Science and Neuroscience (6-9). I am fortunate to have been advised by <a href="http://persci.mit.edu">Ruth Rosenholtz</a> and <a href="https://billf.mit.edu">Bill Freeman</a> in <a href="https://www.csail.mit.edu">CSAIL</a>. Previously, I was an undergradute at the <a href="https://cbmm.mit.edu">Center for Brains, Minds, and Machines</a> and the <a href="https://www.media.mit.edu">Media Lab</a>.
		</p>
		<p style="text-align:center">
		 <img src='images/logos.png' width="400">
		</p>
                <p style="text-align:center">
                  <a href="mailto:anne@yoku-ai.com">Email</a> &nbsp;/&nbsp;
<!--                   <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
<!--                   <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=7M9eSFMAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
		  <a href="https://www.linkedin.com/in/anne-harrington-4a7a04177/">LinkedIn</a> &nbsp;/&nbsp;
<!-- 				  <a href="https://www.threads.net/@jonbarron">Threads</a> &nbsp;/&nbsp;
				  <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/anneharrington/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/harrington_temp-shot.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/harrington_temp-shot.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I want to improve computer vision and machine learning systems by understanding human perception. My interests include robustness, interpretability, computer graphics, and peripheral vision. For my MEng thesis, I explored representational stability and peripheral vision in deep neural networks. As an undergradute, I worked on texture representations as a biological motivation for adversarial robustness. 
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


  <td style="padding:20px;width:25%;vertical-align:middle">
      <img src='images/coco_periph_teaser.png' width="160">
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
<!--     <a href=""> -->
<a href="https://openreview.net/pdf?id=MiRPBbQNHv">
  <span class="papertitle">COCO-Periph: Bridging the Gap Between Human and Machine Perception in the Periphery</span>
</a>
<!--     </a> -->
    <br>
	<strong>Anne Harrington</strong>,
	Vasha DuTell,
	Mark Hamilton,
	Ayush Tewari,
	Simon Stent,
	William T. Freeman,
	Ruth Rosenholtz
    <br>
	<em>In submission</em>
    <br>
    <p></p>
    <p>
    Periphery vision dataset to evalaute and train deep neural networks.
    </p>
  </td>
</tr>   
	    
  <td style="padding:20px;width:25%;vertical-align:middle">
      <img src='images/ATTRIB2.jpg' width="160">
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
<!--     <a href=""> -->
      <span class="papertitle">Object Detection in Deep Neural Networks Differs from Humans in the Periphery
</span>
<!--     </a> -->
    <br>
	<strong>Anne Harrington</strong>,
	Vasha DuTell,
	Mark Hamilton,
	Ayush Tewari,
	Simon Stent,
	William T. Freeman,
	Ruth Rosenholtz
    <br>
	<em>ATTRIB @ NeurIPS</em>, 2023
    <br>
    <p></p>
    <p>
    Psychophysics testing object detection in humans and deep neural networks.
    </p>
  </td>
</tr>          

<!--     <tr onmouseout="camp_stop()" onmouseover="camp_start()"> -->
      <td style="padding:20px;width:25%;vertical-align:middle">
<!--         <div class="one">
          <div class="two" id='camp_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/camp.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='images/Gaze2.jpg' width="160">
<!--         </div> -->
<!--         <script type="text/javascript">
          function camp_start() {
            document.getElementById('camp_image').style.opacity = "1";
          }

          function camp_stop() {
            document.getElementById('camp_image').style.opacity = "0";
          }
          camp_stop() -->
<!--         </script> -->
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openreview.net/forum?id=y5ihmWxYWx">
          <span class="papertitle">Evaluating Peripheral Vision as an Input Transformation to Understand Object Detection Model Behavior</span>
        </a>
        <br>
	<strong>Anne Harrington</strong>,
	Vasha DuTell,
	Mark Hamilton,
	Ayush Tewari,
	Simon Stent,
	William T. Freeman,
	Ruth Rosenholtz
        <br>
        <em>Gaze Meets ML @ NeurIPS</em>, 2023
        <br>
<!--         <a href="https://camp-nerf.github.io/">project page</a> -->
<!--         / -->
<!--         <a href="https://arxiv.org/abs/2308.10902">arXiv</a> -->
        <p></p>
        <p>
        Data augmentation to simulate perpiheral vision in deep neural networks.
        </p>
      </td>
    </tr>

      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='camp_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/camp.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/stat.jpg' width="160">
        </div>
        <script type="text/javascript">
          function camp_start() {
            document.getElementById('camp_image').style.opacity = "1";
          }

          function camp_stop() {
            document.getElementById('camp_image').style.opacity = "0";
          }
          camp_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openreview.net/forum?id=6h8RjNchuh">
          <span class="papertitle">StatTexNet: Evaluating the Importance of Statistical Parameters for Pyramid-Based Texture and Peripheral Vision Models</span>
        </a>
        <br>
	Christian Koevesdi,
	Vasha DuTell,
	<strong>Anne Harrington</strong>,
	Mark Hamilton,
	William T. Freeman,
	Ruth Rosenholtz
        <br>
        <em>Gaze Meets ML @ NeurIPS</em>, 2023
        <br>
<!--         <a href="https://camp-nerf.github.io/">project page</a> -->
<!--         / -->
<!--         <a href="https://arxiv.org/abs/2308.10902">arXiv</a> -->
        <p></p>
        <p>
        Contrastive learning framework to select the most and least important statistics for pyramid-based texture models.
        </p>
      </td>
    </tr> 

      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/straightening1.jpg' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openreview.net/forum?id=4cOfD2qL6T">
          <span class="papertitle">Exploring perceptual straightness in learned visual representations</span>
        </a>
        <br>
	<strong>Anne Harrington</strong>,
	Vasha DuTell,
	Ayush Tewari,
	Mark Hamilton,
	Simon Stent,
	Ruth Rosenholtz,
	William T. Freeman
        <br>
        <em>ICLR</em>, 2023
        <br>
<!--         <a href="https://camp-nerf.github.io/">project page</a> -->
<!--         / -->
<!--         <a href="https://arxiv.org/abs/2308.10902">arXiv</a> -->
        <p></p>
        <p>
        Temporal stability is task dependent and improves with certain types of adversarial robustness.
        </p>
      </td>
    </tr>

      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/straightening3.jpg' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openreview.net/forum?id=A8ucsSFEAqS">
          <span class="papertitle">Exploring the perceptual straightness of adversarially robust and biologically-inspired visual representations</span>
        </a>
        <br>
	<strong>Anne Harrington</strong>,
	Vasha DuTell,
	Ayush Tewari,
	Mark Hamilton,
	Simon Stent,
	Ruth Rosenholtz,
	William T. Freeman
        <br>
        <em>SVRHM @ NeurIPS</em>, 2022
        <br>
<!--         <a href="https://camp-nerf.github.io/">project page</a> -->
<!--         / -->
<!--         <a href="https://arxiv.org/abs/2308.10902">arXiv</a> -->
        <p></p>
        <p>
        Robust models tend to have more temporally stable representations like humans.
        </p>
      </td>
    </tr>
 
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/robust2.jpg' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2202.00838">
          <span class="papertitle">Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks</span>
        </a>
        <br>
	<strong>Anne Harrington</strong> & Arturo Deza
        <br>
        <em>ICLR <strong>spotlight</strong></em>, 2022
        <br>
<!--         <a href="https://camp-nerf.github.io/">project page</a> -->
<!--         / -->
<!--         <a href="https://arxiv.org/abs/2308.10902">arXiv</a> -->
        <p></p>
        <p>
        Adversarially robust features resemble texture representations in peripheral vision.
        </p>
      </td>
    </tr> 

      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/robust2.jpg' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
<!--         <a href="https://openreview.net/forum?id=A8ucsSFEAqS"> -->
          <span class="papertitle">Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks</span>
<!--         </a> -->
        <br>
	<strong>Anne Harrington</strong> & Arturo Deza
        <br>
        <em>SVRHM @ NeurIPS</em>, 2021
        <br>
<!--         <a href="https://camp-nerf.github.io/">project page</a> -->
<!--         / -->
<!--         <a href="https://arxiv.org/abs/2308.10902">arXiv</a> -->
        <p></p>
        <p>
        Workshop version of ICLR paper.
        </p>
      </td>
    </tr> 

      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/sampson.jpg' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376858">
          <span class="papertitle">Interspecies interactions mediated by technology: An avian case study at the zoo</span>
        </a>
        <br>
	R&#233;becca Kleinberger,
	<strong>Anne Harrington</strong>,
	Lydia Yu,
	Akito van Troyer,
	David Su, 
	Janet Barker, 
	Gabriel Miller
        <br>
        <em>CHI</em>, 2020
        <br>
<!--         <a href="https://camp-nerf.github.io/">project page</a> -->
<!--         / -->
<!--         <a href="https://arxiv.org/abs/2308.10902">arXiv</a> -->
        <p></p>
        <p>
        Interactive music device provides sonic enrichment for a macaw.
        </p>
      </td>
    </tr> 
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Forked from Jon Barron's website <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
